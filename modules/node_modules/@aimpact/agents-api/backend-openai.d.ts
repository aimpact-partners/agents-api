/************
Processor: ts
************/

import * as __beyond_dep_ns_0 from 'fs';
import __beyond_dep_def_1 from 'openai';
// buckets\read.ts
declare namespace ns_0 {
  /// <reference types="node" />
  import fs = __beyond_dep_ns_0;
  export function getFile(fileName: string): Promise<fs.ReadStream>;
}


// index.ts
declare namespace ns_1 {
  /// <reference types="node" />
  import OpenAI = __beyond_dep_def_1;
  export class OpenAIBackend {
    #private;
    completions(prompt: string, text: string): Promise<{
      status: boolean;
      data: string;
      error?: undefined;
    } | {
      status: boolean;
      error: any;
      data?: undefined;
    }>;
    chatCompletions(messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[], model?: string, temperature?: number): Promise<{
      status: boolean;
      data: string;
      error?: undefined;
    } | {
      status: boolean;
      error: any;
      data?: undefined;
    }>;
    /**
     *
     * @param path
     * @param lang
     * @returns
     */
    transcription(file: any, lang?: string): Promise<any>;
    transcriptionStream(buffer: Buffer, mimeType: string): Promise<{
      status: boolean;
      data: any;
      error: any;
    } | {
      status: boolean;
      error: any;
      data?: undefined;
    }>;
  }
}


export import OpenAIBackend = ns_1.OpenAIBackend;

export declare const hmr: {on: (event: string, listener: any) => void, off: (event: string, listener: any) => void };